
# Overview
GPT-3.5/4 带给我们无限震撼的同时，其天然的缺陷和诸多的限制也让开发者头痛不已

其中最主要的问题就是他对上下文的理解能力不足，他最多只能理解3000个字左右的上下文，gpt4能理解大概6000个字左右的上下文，也就是说它的记忆能力只有6000字。

当然现在GPT也在不断的发展，目前最大的模型大概能支持十万字的记忆，你可能对十万字没概念，但是我举个例子你就差不多明白了，大概是一部哈利波特的字数。

甚至现在谷歌推出了一个模型，能支持最多100万字的记忆能力，大概是一个1500字左右的文档。

这个记忆能力其实已经挺夸张的了，至少对于个人用户，这个记忆能力已经是完全够用的了，甚至你可以说他的记忆能力在某些方面比人要强。

但是这种能力，在面对工业上的生产时，就显得有点，不太够用了。

而且还有一个比较严重的问题，就是GPT的计费时按照字数计费的，所以如果你每次都把记忆全部给他，那么导致的问题就是你会花比较多的钱。

# Possible Solution
那么怎么解决这个问题呢？

第一个解法就是，我们提升模型的记忆能力，如果模型的记忆能力达到千万级或者亿级，那么这个问题自然迎刃而解，业界目前的研究是通过加入一些位置信息或者编码来做的


[Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595)

[YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)


当然，还有另外一种解法，我个人认为这种解法更像人类的记忆能力，




**为什么我们需要embedding？**

比如你要搜索一个帅哥，那么你只能搜到一个广义的帅哥，并不能搜到那个基于你的记忆推算出来的，最适合你的帅哥。


例如，如果你搜索“小狗”，那么你只能得到带有“小狗”关键字相关的结果，而无法得到“柯基”、“金毛”等结果，因为“小狗”和“金毛”是不同的词，传统数据库无法识别它们的语义关系，所以传统的应用需要人为的将“小狗”和“金毛”等词之间打上特征标签进行关联，这样才能实现语义搜索。

我们先思考一个问题？为什么我们在生活中区分不同的物品和事物？

如果从理论角度出发，这是因为我们会通过识别不同事物之间不同的特征来识别种类，例如分别不同种类的小狗，就可以通过体型大小、毛发长度、鼻子长短等特征来区分。如下面这张照片按照体型排序，可以看到体型越大的狗越靠近坐标轴右边，这样就能得到一个体型特征的一维坐标和对应的数值，从 0 到 1 的数字中得到每只狗在坐标系中的位置

然而单靠一个体型大小的特征并不够，像照片中哈士奇、金毛和拉布拉多的体型就非常接近，我们无法区分。所以我们会继续观察其它的特征，例如毛发的长短。

这样每只狗对应一个二维坐标点，我们就能轻易的将哈士奇、金毛和拉布拉多区分开来，如果这时仍然无法很好的区分德牧和罗威纳犬。我们就可以继续再从其它的特征区分，比如鼻子的长短，这样就能得到一个三维的坐标系和每只狗在三维坐标系中的位置。


**Query**

我们知道了可以通过比较向量之间的距离来判断它们的相似度, 那么如何将它应用到真实的场景中呢？

那这和相似性搜索 (Similarity Search) 有什么关系呢？你会发现在上面的二维坐标中，德牧和罗威纳犬的坐标就非常接近，这就意味着它们的特征也非常接近。我们都知道向量是具有大小和方向的数学结构，所以可以将这些特征用向量来表示，这样就能够通过计算向量之间的距离来判断它们的相似度，这就是相似性搜索。

理论上来说，当你的维度足够高，你可以分开所有东西。


同时这里还有一个问题，你的搜索效率会很低，比如你的数据库中有100万条数据，那么你要找到最相似的那一个，你每次都得全看一遍，这其实是很耗费资源的。


那么怎么解决这个问题呢，其实常用的算法是聚类算法。

**Kmeans**

我们可以在保存向量数据后，先对向量数据先进行聚类。例如下图在二维坐标系中，划定了 4 个聚类中心，然后将每个向量分配到最近的聚类中心，经过聚类算法不断调整聚类中心位置，这样就可以将向量数据分成 4 个簇。每次搜索时，只需要先判断搜索向量属于哪个簇，然后再在这一个簇中进行搜索，这样就从 4 个簇的搜索范围减少到了 1 个簇，大大减少了搜索的范围。

**PQ**

但问题是，我们之前讨论过，有一个纬度诅咒的问题，也就是随着维度的提升，你想要填充满这个空间所要花费的资源会变得超级多，你可以想象一下，用点铺满二维空间个3维空间，点的个数其实是在指数增长的。















